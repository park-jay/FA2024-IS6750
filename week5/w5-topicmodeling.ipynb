{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* We learned how to count N-grams (i.e., sequences of N words such as unigrams (N=1) and bigrams (N=2)) by tokenizing the text.\n",
    ">* But we faced a problem of counting N-grams when there are unnecessary or meaningless tokens after tokenization.\n",
    ">* Therefore, we needed a further processing to remove stopwords (i.e., function words) and punctuations. \n",
    ">* The last step was to convert the text into lemma form (i.e., the base form of words) to avoid the duplication of the same word with different forms. For instance, 'running' and 'ran' are converted into 'run' because they have the same meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* In this week, we will learn topic modeling, one of the unsupervised learning techniques, to extract topics from the text. \n",
    ">* Topic modeling is a type of statistical model to discover abstract topics that occur in a collection of documents. \n",
    ">* We will use the Latent Dirichlet Allocation (LDA) model, one of the most popular topic modeling techniques. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* We are going to use the `nltk' library for tokenization and stopwords removal.\n",
    ">* We will use the `gensim' library for topic modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's import the data from week 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('../week3/Political-media-DFE.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['_unit_id', '_golden', '_unit_state', '_trusted_judgments',\n",
       "       '_last_judgment_at', 'audience', 'audience:confidence', 'bias',\n",
       "       'bias:confidence', 'message', 'message:confidence', 'orig__golden',\n",
       "       'audience_gold', 'bias_gold', 'bioid', 'embed', 'id', 'label',\n",
       "       'message_gold', 'source', 'text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_unit_id                 int64\n",
       "_golden                   bool\n",
       "_unit_state             object\n",
       "_trusted_judgments       int64\n",
       "_last_judgment_at       object\n",
       "audience                object\n",
       "audience:confidence    float64\n",
       "bias                    object\n",
       "bias:confidence        float64\n",
       "message                 object\n",
       "message:confidence     float64\n",
       "orig__golden           float64\n",
       "audience_gold          float64\n",
       "bias_gold              float64\n",
       "bioid                   object\n",
       "embed                   object\n",
       "id                      object\n",
       "label                   object\n",
       "message_gold           float64\n",
       "source                  object\n",
       "text                    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* `dtypes' is used to check the data type of each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's subset the data to have who posted, where they posed (social media platform), and what they posted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "content=data[['label', 'source', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: Trey Radel (Representative from Florida)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: Mitch McConnell (Senator from Kentucky)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>VIDEO - #Obamacare:  Full of Higher Costs and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: Kurt Schrader (Representative from Oregon)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>Please join me today in remembering our fallen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: Michael Crapo (Senator from Idaho)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>RT @SenatorLeahy: 1st step toward Senate debat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: Mark Udall (Senator from Colorado)</td>\n",
       "      <td>twitter</td>\n",
       "      <td>.@amazon delivery #drones show need to update ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>From: Ted Yoho (Representative from Florida)</td>\n",
       "      <td>facebook</td>\n",
       "      <td>I applaud Governor PerryÛªs recent decision t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>From: Ted Yoho (Representative from Florida)</td>\n",
       "      <td>facebook</td>\n",
       "      <td>Today, I voted in favor of H.R. 5016 - Financi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>From: Ted Yoho (Representative from Florida)</td>\n",
       "      <td>facebook</td>\n",
       "      <td>(Taken from posted WOKV interview)   Congressm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>From: Ted Yoho (Representative from Florida)</td>\n",
       "      <td>facebook</td>\n",
       "      <td>Join me next week for a town hall in Ocala! I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>From: Ted Yoho (Representative from Florida)</td>\n",
       "      <td>facebook</td>\n",
       "      <td>Foreign Affairs Committee Hearing on Syria. I ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 label    source  \\\n",
       "0       From: Trey Radel (Representative from Florida)   twitter   \n",
       "1        From: Mitch McConnell (Senator from Kentucky)   twitter   \n",
       "2     From: Kurt Schrader (Representative from Oregon)   twitter   \n",
       "3             From: Michael Crapo (Senator from Idaho)   twitter   \n",
       "4             From: Mark Udall (Senator from Colorado)   twitter   \n",
       "...                                                ...       ...   \n",
       "4995      From: Ted Yoho (Representative from Florida)  facebook   \n",
       "4996      From: Ted Yoho (Representative from Florida)  facebook   \n",
       "4997      From: Ted Yoho (Representative from Florida)  facebook   \n",
       "4998      From: Ted Yoho (Representative from Florida)  facebook   \n",
       "4999      From: Ted Yoho (Representative from Florida)  facebook   \n",
       "\n",
       "                                                   text  \n",
       "0     RT @nowthisnews: Rep. Trey Radel (R- #FL) slam...  \n",
       "1     VIDEO - #Obamacare:  Full of Higher Costs and ...  \n",
       "2     Please join me today in remembering our fallen...  \n",
       "3     RT @SenatorLeahy: 1st step toward Senate debat...  \n",
       "4     .@amazon delivery #drones show need to update ...  \n",
       "...                                                 ...  \n",
       "4995  I applaud Governor PerryÛªs recent decision t...  \n",
       "4996  Today, I voted in favor of H.R. 5016 - Financi...  \n",
       "4997  (Taken from posted WOKV interview)   Congressm...  \n",
       "4998  Join me next week for a town hall in Ocala! I'...  \n",
       "4999  Foreign Affairs Committee Hearing on Syria. I ...  \n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's print out the text data in the first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @nowthisnews: Rep. Trey Radel (R- #FL) slams #Obamacare. #politics https://t.co/zvywMG8yIH'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* If you want to check the text data in the fifth row, you can use `df['text'][4]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.@amazon delivery #drones show need to update law to promote #innovation &amp; protect #privacy. My #UAS bill does that: http://t.co/l9ta5SKq6u'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['text'].iloc[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's clean the data for LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* First step is to lowercase the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/r8z51q092v566bd_r0g_zj640000gn/T/ipykernel_1937/2949361602.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  content['text-lower']=content['text'].str.lower()\n"
     ]
    }
   ],
   "source": [
    "content['text-lower']=content['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RT @nowthisnews: Rep. Trey Radel (R- #FL) slams #Obamacare. #politics https://t.co/zvywMG8yIH'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rt @nowthisnews: rep. trey radel (r- #fl) slams #obamacare. #politics https://t.co/zvywmg8yih'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['text-lower'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * We can seperate the entire contents into tokens (words, hashtags, mentions, etc.).\n",
    "> * Seperating the contents into tokens is called tokenization.\n",
    "> * We can use the `word_tokenize` function from the `nltk` library to tokenize the contents.\n",
    "> * There is also a `TweetTokenizer` function in the `nltk` library that is specifically for tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * `.apply` is used to apply a function to a column. You don't have to use a for loop to apply a function to each row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * There are two ways to tokenize the contents. One is to use `apply()` function to tokenize the lowercased text. \n",
    "> * `apply()` function allows you to apply a function along the axis of a DataFrame.\n",
    "> * Another way is to iterate through the lowercased text and tokenize each content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/r8z51q092v566bd_r0g_zj640000gn/T/ipykernel_1937/2884168308.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  content['tokenized_unigrams']=content['text-lower'].apply(word_tokenize)\n"
     ]
    }
   ],
   "source": [
    "content['tokenized_unigrams']=content['text-lower'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterated_unigrams=[]\n",
    "for idx, row in content.iterrows():\n",
    "    iterated_unigrams.append(word_tokenize(row['text-lower']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/r8z51q092v566bd_r0g_zj640000gn/T/ipykernel_1937/1711205816.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  content['iterated_unigrams']=iterated_unigrams\n"
     ]
    }
   ],
   "source": [
    "content['iterated_unigrams']=iterated_unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * The results of iterating through each row and applying the `word_tokenize` function is a list of lists are identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.loc[0,'iterated_unigrams'] == content.loc[0,'tokenized_unigrams']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * In the `nltk` library, there is a list of stopwords (function words) that we can use to remove from the contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop[1:10] #use slice to show only the first 10 stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop[-10:] #use negative index to slice the last 10 stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Because stopwords list can be customized, we can add or remove words from the list.\n",
    ">* Considering some of the data is collected from Twitter, we can add some Twitter-specific stopwords like `rt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop.append('rt') #add 'rt' to the stopwords list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* We want to remove the stopwords from the text-lower column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/r8z51q092v566bd_r0g_zj640000gn/T/ipykernel_1937/1207516474.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  content['stopword']=content['text-lower'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))\n"
     ]
    }
   ],
   "source": [
    "content['stopword']=content['text-lower'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))\n",
    "#The lambda function takes each row of the 'text-lower' column, splits it into a list of words, \n",
    "#and then joins the words back together into a string, excluding any words that are in the 'stop' list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's get rid of irrelevant punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/r8z51q092v566bd_r0g_zj640000gn/T/ipykernel_1937/445710354.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  content['stop_tokenized_unigrams']=content['stopword'].apply(word_tokenize)\n"
     ]
    }
   ],
   "source": [
    "content['stop_tokenized_unigrams']=content['stopword'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/r8z51q092v566bd_r0g_zj640000gn/T/ipykernel_1937/3655593217.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  content['punct_tokenized_unigrams']=content['stop_tokenized_unigrams'].apply(lambda x: [word for word in x if word.isalnum()])\n"
     ]
    }
   ],
   "source": [
    "content['punct_tokenized_unigrams']=content['stop_tokenized_unigrams'].apply(lambda x: [word for word in x if word.isalnum()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* The last step is to convert the text data into lemma form.\n",
    ">* When counting the most frequent words, we saw that the same word with different forms was counted separately. For instance, the past and present tense of the same word were counted as two different words.\n",
    ">* To avoid this, we will use lemmatization to convert the text data into the base form of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Interestingly enough, NLTK's WordNetLemmatizer is not perfect.\n",
    "> * By default, it only lemmatize nouns.\n",
    "> * Therefore, we need to specify the part of speech (POS) for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'): #ADJECTIVE\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'): #VERB\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'): #NOUN        \n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'): #ADVERB\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence(sentence):\n",
    "    # Tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    # Tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged) \n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            # If no tag was found, then use the word as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            # Else use the tag to lemmatize the word\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/r8z51q092v566bd_r0g_zj640000gn/T/ipykernel_1937/3910947984.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  content['lemma']=content['punct_tokenized_unigrams'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])\n"
     ]
    }
   ],
   "source": [
    "content['lemma']=content['punct_tokenized_unigrams'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/r8z51q092v566bd_r0g_zj640000gn/T/ipykernel_1937/1287760357.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  content['lemmatizer_str']=content['lemma'].apply(lambda x: lemmatize_sentence(' '.join(x)))\n"
     ]
    }
   ],
   "source": [
    "content['lemmatizer_str']=content['lemma'].apply(lambda x: lemmatize_sentence(' '.join(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/r8z51q092v566bd_r0g_zj640000gn/T/ipykernel_1937/2752087146.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  content['lemmatizer_token']=content['lemmatizer_str'].apply(word_tokenize)\n"
     ]
    }
   ],
   "source": [
    "content['lemmatizer_token']=content['lemmatizer_str'].apply(word_tokenize)\n",
    "#tokenize the corrected lemmatized string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Let's compare the results of lemmatization and without lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'call usdotfra release info inspection casseltonderailment review quality rail'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.loc[5, 'lemmatizer_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'called on the @usdotfra to release info about inspections before the #casseltonderailment to review quality of rails. (1/2)'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.loc[5, 'text-lower']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* We will import the `gensim` library for topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* In order for preparing the text data for LDA, we need to create a dictionary and a corpus.\n",
    ">* A dictionary is a mapping between words and their integer ids.\n",
    ">* A corpus is a list of lists where each list represents the bag of words for a single document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word=corpora.Dictionary(content['lemmatizer_token'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[id2word.doc2bow(text) for text in content['lemmatizer_token']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0] \n",
    "#the first element in the tuple is the word id, \n",
    "#and the second element is the frequency of the word in the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['nowthisnews', 'trey', 'radel', 'fl', 'slam', 'obamacare', 'politics', 'http']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['lemmatizer_token'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['photo',\n",
       " 'join',\n",
       " 'world',\n",
       " 'interest',\n",
       " 'man',\n",
       " 'from',\n",
       " 'america',\n",
       " 'interest',\n",
       " 'state',\n",
       " 'landmines',\n",
       " 'event',\n",
       " 'cap',\n",
       " 'hill',\n",
       " 'http']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Another example for the frequency of the word is two ('interest')\n",
    "content['lemmatizer_token'][91]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 1),\n",
       " (18, 1),\n",
       " (78, 1),\n",
       " (92, 1),\n",
       " (360, 1),\n",
       " (436, 1),\n",
       " (491, 1),\n",
       " (732, 1),\n",
       " (733, 1),\n",
       " (734, 1),\n",
       " (735, 2),\n",
       " (736, 1),\n",
       " (737, 1)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[91]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Q. What is the index number of the word 'interest'?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* The answer is 735. Note that the order of tuples in `corpus` does not follow the order of tokens.\n",
    ">* Gensim creates a unique id for each word in the document. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'interest'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[735]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Q. What is the word of index number 1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* The answer is 'http' because it appeared in the first instance of the `content['lemmatizer_token']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Now we build the model with the dictionary and the corpus.\n",
    ">* There are several parameters to set for the LDA model.\n",
    ">* The number of topics is one of the most important parameters to set. It will be specified under `num_topics`.\n",
    ">* We can set the number of topics to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda=LdaModel(corpus=corpus, id2word=id2word, num_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "#This is just to show the topics in a more readable way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.016*\"http\" + 0.007*\"obamacare\" + 0.007*\"today\" + 0.006*\"know\" + '\n",
      "  '0.005*\"click\" + 0.005*\"health\" + 0.005*\"congressional\" + 0.005*\"house\" + '\n",
      "  '0.005*\"take\" + 0.005*\"care\"'),\n",
      " (1,\n",
      "  '0.016*\"http\" + 0.009*\"today\" + 0.007*\"new\" + 0.007*\"job\" + 0.007*\"house\" + '\n",
      "  '0.006*\"american\" + 0.006*\"time\" + 0.006*\"family\" + 0.006*\"great\" + '\n",
      "  '0.006*\"act\"'),\n",
      " (2,\n",
      "  '0.014*\"http\" + 0.009*\"make\" + 0.008*\"president\" + 0.007*\"great\" + '\n",
      "  '0.007*\"today\" + 0.006*\"work\" + 0.005*\"sander\" + 0.005*\"get\" + 0.004*\"like\" '\n",
      "  '+ 0.004*\"year\"'),\n",
      " (3,\n",
      "  '0.014*\"veteran\" + 0.007*\"http\" + 0.007*\"today\" + 0.007*\"state\" + '\n",
      "  '0.005*\"day\" + 0.005*\"family\" + 0.005*\"u\" + 0.004*\"service\" + 0.004*\"get\" + '\n",
      "  '0.004*\"american\"'),\n",
      " (4,\n",
      "  '0.008*\"http\" + 0.006*\"today\" + 0.006*\"work\" + 0.005*\"help\" + 0.005*\"school\" '\n",
      "  '+ 0.005*\"day\" + 0.005*\"service\" + 0.005*\"executive\" + 0.004*\"american\" + '\n",
      "  '0.004*\"get\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* But you'll find that the topic words will change every time you run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda=LdaModel(corpus=corpus, id2word=id2word, num_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.016*\"http\" + 0.007*\"year\" + 0.007*\"today\" + 0.006*\"say\" + 0.005*\"get\" + '\n",
      "  '0.005*\"congress\" + 0.005*\"office\" + 0.005*\"house\" + 0.005*\"american\" + '\n",
      "  '0.004*\"president\"'),\n",
      " (1,\n",
      "  '0.015*\"http\" + 0.009*\"house\" + 0.008*\"veteran\" + 0.007*\"bill\" + '\n",
      "  '0.005*\"work\" + 0.005*\"american\" + 0.005*\"today\" + 0.005*\"new\" + '\n",
      "  '0.005*\"congress\" + 0.005*\"tax\"'),\n",
      " (2,\n",
      "  '0.011*\"work\" + 0.010*\"make\" + 0.010*\"http\" + 0.008*\"great\" + 0.007*\"family\" '\n",
      "  '+ 0.007*\"woman\" + 0.005*\"business\" + 0.005*\"american\" + 0.005*\"year\" + '\n",
      "  '0.005*\"day\"'),\n",
      " (3,\n",
      "  '0.013*\"http\" + 0.012*\"today\" + 0.008*\"school\" + 0.007*\"great\" + '\n",
      "  '0.007*\"president\" + 0.006*\"law\" + 0.006*\"service\" + 0.006*\"visit\" + '\n",
      "  '0.005*\"new\" + 0.005*\"high\"'),\n",
      " (4,\n",
      "  '0.012*\"today\" + 0.010*\"http\" + 0.009*\"day\" + 0.006*\"happy\" + 0.006*\"state\" '\n",
      "  '+ 0.006*\"here\" + 0.005*\"military\" + 0.005*\"family\" + 0.005*\"great\" + '\n",
      "  '0.005*\"de\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* This is because the LDA model is a probabilistic model, so the results are not deterministic.\n",
    ">* To get the same results, you need to set the seed number.\n",
    ">* The seed number is set under `random_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda=LdaModel(corpus=corpus, id2word=id2word, num_topics=5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.011*\"http\" + 0.008*\"day\" + 0.008*\"work\" + 0.008*\"time\" + 0.007*\"act\" + '\n",
      "  '0.007*\"government\" + 0.007*\"health\" + 0.007*\"care\" + 0.007*\"vote\" + '\n",
      "  '0.006*\"house\"'),\n",
      " (1,\n",
      "  '0.011*\"http\" + 0.009*\"today\" + 0.008*\"great\" + 0.007*\"state\" + 0.006*\"job\" '\n",
      "  '+ 0.006*\"school\" + 0.005*\"american\" + 0.005*\"new\" + 0.005*\"help\" + '\n",
      "  '0.005*\"service\"'),\n",
      " (2,\n",
      "  '0.016*\"http\" + 0.011*\"today\" + 0.006*\"house\" + 0.005*\"new\" + '\n",
      "  '0.005*\"veteran\" + 0.005*\"year\" + 0.005*\"american\" + 0.004*\"help\" + '\n",
      "  '0.004*\"great\" + 0.004*\"need\"'),\n",
      " (3,\n",
      "  '0.009*\"http\" + 0.009*\"work\" + 0.007*\"would\" + 0.007*\"law\" + 0.006*\"make\" + '\n",
      "  '0.006*\"today\" + 0.006*\"tax\" + 0.005*\"keep\" + 0.005*\"legislation\" + '\n",
      "  '0.004*\"day\"'),\n",
      " (4,\n",
      "  '0.013*\"http\" + 0.006*\"family\" + 0.005*\"please\" + 0.005*\"work\" + '\n",
      "  '0.005*\"great\" + 0.004*\"de\" + 0.004*\"make\" + 0.004*\"today\" + 0.004*\"year\" + '\n",
      "  '0.004*\"hall\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* You can also increase the number of topics by changing the `num_topics` parameter.\n",
    ">* Let's set the number of topics to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda=LdaModel(corpus=corpus, id2word=id2word, num_topics=10, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.014*\"day\" + 0.013*\"http\" + 0.010*\"first\" + 0.010*\"happy\" + 0.009*\"hall\" + '\n",
      "  '0.009*\"town\" + 0.007*\"point\" + 0.007*\"ii\" + 0.007*\"year\" + 0.006*\"change\"'),\n",
      " (1,\n",
      "  '0.017*\"http\" + 0.010*\"great\" + 0.008*\"affair\" + 0.008*\"today\" + '\n",
      "  '0.008*\"school\" + 0.008*\"meet\" + 0.008*\"share\" + 0.006*\"continue\" + '\n",
      "  '0.006*\"american\" + 0.006*\"park\"'),\n",
      " (2,\n",
      "  '0.019*\"http\" + 0.013*\"today\" + 0.008*\"great\" + 0.007*\"year\" + 0.007*\"new\" + '\n",
      "  '0.006*\"sander\" + 0.006*\"visit\" + 0.006*\"state\" + 0.005*\"i\" + '\n",
      "  '0.005*\"family\"'),\n",
      " (3,\n",
      "  '0.010*\"http\" + 0.008*\"must\" + 0.008*\"governor\" + 0.007*\"president\" + '\n",
      "  '0.007*\"legislation\" + 0.007*\"work\" + 0.006*\"today\" + 0.006*\"mental\" + '\n",
      "  '0.005*\"december\" + 0.005*\"del\"'),\n",
      " (4,\n",
      "  '0.017*\"http\" + 0.009*\"family\" + 0.007*\"click\" + 0.007*\"immigration\" + '\n",
      "  '0.007*\"de\" + 0.006*\"today\" + 0.006*\"reform\" + 0.006*\"great\" + 0.006*\"night\" '\n",
      "  '+ 0.005*\"government\"'),\n",
      " (5,\n",
      "  '0.018*\"law\" + 0.015*\"http\" + 0.013*\"health\" + 0.012*\"care\" + '\n",
      "  '0.011*\"president\" + 0.009*\"american\" + 0.009*\"day\" + 0.008*\"congress\" + '\n",
      "  '0.008*\"today\" + 0.008*\"house\"'),\n",
      " (6,\n",
      "  '0.012*\"http\" + 0.010*\"work\" + 0.010*\"time\" + 0.010*\"house\" + 0.010*\"bill\" + '\n",
      "  '0.009*\"make\" + 0.009*\"act\" + 0.009*\"job\" + 0.008*\"today\" + 0.007*\"senate\"'),\n",
      " (7,\n",
      "  '0.012*\"work\" + 0.009*\"know\" + 0.009*\"business\" + 0.008*\"american\" + '\n",
      "  '0.008*\"veteran\" + 0.008*\"people\" + 0.007*\"one\" + 0.007*\"provide\" + '\n",
      "  '0.006*\"http\" + 0.006*\"represent\"'),\n",
      " (8,\n",
      "  '0.013*\"money\" + 0.011*\"http\" + 0.008*\"congressman\" + 0.008*\"paso\" + '\n",
      "  '0.007*\"congresswoman\" + 0.007*\"year\" + 0.007*\"national\" + 0.006*\"nsa\" + '\n",
      "  '0.006*\"monday\" + 0.006*\"el\"'),\n",
      " (9,\n",
      "  '0.015*\"http\" + 0.008*\"u\" + 0.008*\"today\" + 0.008*\"state\" + 0.008*\"service\" '\n",
      "  '+ 0.007*\"house\" + 0.007*\"say\" + 0.006*\"new\" + 0.006*\"veteran\" + '\n",
      "  '0.005*\"committee\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Q. Do you like the results of the LDA model? What do you find interesting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Finding the optimal number of topics is a challenging task.\n",
    ">* The topic coherence score and perplexity are two common metrics to evaluate the model.\n",
    ">* But this is beyond the scope of this course. Come find me if you're interested in learning more about it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* The assumption of the LDA model is that each document is a mixture of topics.\n",
    ">* In this case with social media data, the document is an individual post and the topics are the themes of the post.\n",
    ">* However, given that the social media data is too short to have a mixture of topics, the LDA model may not work well.\n",
    ">* The LDA model is more suitable for long documents like research papers, articles, and books.\n",
    ">* Therefore, researchers developed another topic model called NMF (Non-negative Matrix Factorization) for short argumentative texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's learn NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.nmf import Nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = Nmf(corpus=corpus, id2word=id2word, num_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.069*\"http\" + 0.009*\"student\" + 0.008*\"day\" + 0.007*\"happy\" + 0.007*\"gop\" '\n",
      "  '+ 0.006*\"talk\" + 0.006*\"week\" + 0.005*\"icymi\" + 0.005*\"debt\" + '\n",
      "  '0.005*\"watch\"'),\n",
      " (1,\n",
      "  '0.014*\"job\" + 0.012*\"business\" + 0.010*\"http\" + 0.009*\"year\" + 0.009*\"amp\" '\n",
      "  '+ 0.008*\"i\" + 0.008*\"day\" + 0.008*\"help\" + 0.007*\"week\" + 0.007*\"today\"'),\n",
      " (2,\n",
      "  '0.030*\"law\" + 0.015*\"congress\" + 0.013*\"president\" + 0.012*\"get\" + '\n",
      "  '0.011*\"say\" + 0.009*\"do\" + 0.009*\"change\" + 0.008*\"go\" + 0.008*\"today\" + '\n",
      "  '0.008*\"work\"'),\n",
      " (3,\n",
      "  '0.010*\"law\" + 0.010*\"work\" + 0.010*\"need\" + 0.009*\"american\" + 0.009*\"make\" '\n",
      "  '+ 0.007*\"act\" + 0.007*\"bill\" + 0.007*\"vote\" + 0.007*\"tax\" + 0.007*\"family\"'),\n",
      " (4,\n",
      "  '0.119*\"http\" + 0.031*\"amp\" + 0.011*\"bill\" + 0.009*\"great\" + 0.008*\"vote\" + '\n",
      "  '0.008*\"join\" + 0.007*\"house\" + 0.005*\"via\" + 0.005*\"2\" + 0.005*\"meet\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(nmf.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Similar to LDA, NMF returns the topic words each time you run the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = Nmf(corpus=corpus, id2word=id2word, num_topics=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.031*\"amp\" + 0.029*\"http\" + 0.018*\"vote\" + 0.012*\"today\" + 0.009*\"bill\" + '\n",
      "  '0.008*\"join\" + 0.008*\"family\" + 0.007*\"u\" + 0.007*\"year\" + 0.006*\"student\"'),\n",
      " (1,\n",
      "  '0.020*\"house\" + 0.018*\"http\" + 0.016*\"new\" + 0.012*\"president\" + '\n",
      "  '0.011*\"job\" + 0.007*\"here\" + 0.007*\"read\" + 0.006*\"support\" + 0.006*\"obama\" '\n",
      "  '+ 0.005*\"work\"'),\n",
      " (2,\n",
      "  '0.021*\"today\" + 0.012*\"http\" + 0.011*\"act\" + 0.010*\"health\" + 0.009*\"great\" '\n",
      "  '+ 0.008*\"day\" + 0.007*\"care\" + 0.007*\"thanks\" + 0.006*\"state\" + '\n",
      "  '0.006*\"service\"'),\n",
      " (3,\n",
      "  '0.030*\"law\" + 0.016*\"congress\" + 0.010*\"say\" + 0.010*\"get\" + 0.010*\"work\" + '\n",
      "  '0.010*\"make\" + 0.009*\"do\" + 0.009*\"people\" + 0.008*\"change\" + 0.008*\"go\"'),\n",
      " (4,\n",
      "  '0.128*\"http\" + 0.010*\"amp\" + 0.009*\"business\" + 0.008*\"gop\" + 0.008*\"small\" '\n",
      "  '+ 0.006*\"american\" + 0.006*\"tax\" + 0.006*\"budget\" + 0.005*\"obamacare\" + '\n",
      "  '0.005*\"via\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(nmf.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* To avoid this randomness, you can set the seed number under `random_state`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf=Nmf(corpus=corpus, id2word=id2word, num_topics=5, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.037*\"http\" + 0.029*\"today\" + 0.014*\"great\" + 0.013*\"day\" + 0.010*\"state\" '\n",
      "  '+ 0.006*\"time\" + 0.006*\"here\" + 0.006*\"new\" + 0.006*\"read\" + 0.005*\"year\"'),\n",
      " (1,\n",
      "  '0.024*\"http\" + 0.021*\"job\" + 0.017*\"american\" + 0.016*\"get\" + 0.014*\"work\" '\n",
      "  '+ 0.010*\"people\" + 0.010*\"law\" + 0.007*\"would\" + 0.006*\"economy\" + '\n",
      "  '0.006*\"rate\"'),\n",
      " (2,\n",
      "  '0.038*\"law\" + 0.021*\"congress\" + 0.014*\"president\" + 0.012*\"say\" + '\n",
      "  '0.011*\"do\" + 0.010*\"change\" + 0.010*\"get\" + 0.009*\"go\" + 0.009*\"thing\" + '\n",
      "  '0.009*\"executive\"'),\n",
      " (3,\n",
      "  '0.010*\"make\" + 0.009*\"vote\" + 0.007*\"house\" + 0.007*\"work\" + 0.007*\"tax\" + '\n",
      "  '0.006*\"act\" + 0.006*\"u\" + 0.006*\"year\" + 0.006*\"legislation\" + '\n",
      "  '0.006*\"need\"'),\n",
      " (4,\n",
      "  '0.109*\"http\" + 0.030*\"amp\" + 0.009*\"hear\" + 0.009*\"gop\" + 0.006*\"support\" + '\n",
      "  '0.005*\"talk\" + 0.005*\"floor\" + 0.004*\"tune\" + 0.004*\"family\" + '\n",
      "  '0.004*\"house\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(nmf.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* You can also increase the number of topics by changing the `n_topics` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf=Nmf(corpus=corpus, id2word=id2word, num_topics=10, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.020*\"bill\" + 0.019*\"veteran\" + 0.018*\"today\" + 0.008*\"service\" + '\n",
      "  '0.008*\"va\" + 0.008*\"honor\" + 0.008*\"congress\" + 0.007*\"new\" + '\n",
      "  '0.007*\"family\" + 0.007*\"president\"'),\n",
      " (1,\n",
      "  '0.017*\"job\" + 0.010*\"work\" + 0.010*\"make\" + 0.010*\"american\" + 0.010*\"tax\" '\n",
      "  '+ 0.009*\"house\" + 0.009*\"would\" + 0.009*\"year\" + 0.008*\"legislation\" + '\n",
      "  '0.008*\"business\"'),\n",
      " (2,\n",
      "  '0.013*\"state\" + 0.012*\"http\" + 0.011*\"time\" + 0.010*\"school\" + '\n",
      "  '0.010*\"obamacare\" + 0.009*\"day\" + 0.008*\"national\" + 0.008*\"here\" + '\n",
      "  '0.007*\"year\" + 0.007*\"live\"'),\n",
      " (3,\n",
      "  '0.022*\"today\" + 0.020*\"great\" + 0.014*\"get\" + 0.014*\"vote\" + 0.009*\"go\" + '\n",
      "  '0.009*\"law\" + 0.007*\"hear\" + 0.007*\"talk\" + 0.007*\"make\" + 0.007*\"one\"'),\n",
      " (4,\n",
      "  '0.049*\"law\" + 0.021*\"congress\" + 0.016*\"president\" + 0.016*\"say\" + '\n",
      "  '0.015*\"do\" + 0.013*\"change\" + 0.012*\"go\" + 0.012*\"get\" + 0.012*\"thing\" + '\n",
      "  '0.011*\"make\"'),\n",
      " (5,\n",
      "  '0.107*\"http\" + 0.020*\"help\" + 0.013*\"here\" + 0.010*\"health\" + 0.010*\"watch\" '\n",
      "  '+ 0.009*\"bill\" + 0.008*\"american\" + 0.008*\"student\" + 0.007*\"business\" + '\n",
      "  '0.006*\"small\"'),\n",
      " (6,\n",
      "  '0.051*\"http\" + 0.028*\"house\" + 0.027*\"work\" + 0.021*\"great\" + 0.011*\"week\" '\n",
      "  '+ 0.010*\"family\" + 0.009*\"act\" + 0.009*\"new\" + 0.007*\"support\" + '\n",
      "  '0.007*\"protect\"'),\n",
      " (7,\n",
      "  '0.101*\"http\" + 0.016*\"obama\" + 0.014*\"budget\" + 0.013*\"thanks\" + '\n",
      "  '0.012*\"gop\" + 0.011*\"make\" + 0.009*\"government\" + 0.008*\"shutdown\" + '\n",
      "  '0.007*\"time\" + 0.007*\"today\"'),\n",
      " (8,\n",
      "  '0.037*\"http\" + 0.020*\"amp\" + 0.018*\"tax\" + 0.018*\"u\" + 0.016*\"hall\" + '\n",
      "  '0.014*\"join\" + 0.013*\"town\" + 0.008*\"credit\" + 0.007*\"infrastructure\" + '\n",
      "  '0.005*\"community\"'),\n",
      " (9,\n",
      "  '0.061*\"amp\" + 0.024*\"veteran\" + 0.014*\"job\" + 0.011*\"http\" + '\n",
      "  '0.008*\"service\" + 0.008*\"help\" + 0.006*\"look\" + 0.006*\"proud\" + '\n",
      "  '0.006*\"office\" + 0.006*\"va\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(nmf.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Q. Which model do you like better? LDA or NMF?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Topic modeling helps you understand the overall abstract level of the text data.\n",
    ">* When you want to know about what the text is about, you can use topic modeling to extract the topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* We are moving on to the word level understanding of the text data.\n",
    ">* The words that appear together frequently are likely to have a strong relationship.\n",
    ">* With this we can understand how the word was used in the context.\n",
    ">* Word embedding is a technique to represent words in a relation to other words in the text data.\n",
    ">* To do so, each word is represented as a dense vector in a high-dimensional space.\n",
    ">* The distance between the vectors represents the relationship between the words.\n",
    ">* Similar words will be located close to each other in the vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Before we plug in week 3 data, let's learn key functions in the Word2Vec model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's download `word2vec-google-news-300` from the `gensim` library.\n",
    ">* This dataset is pre-trained on Google News data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector=gensim.downloader.load('word2vec-google-news-300')\n",
    "#the size is 1662.8 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's look at the most similar words to the word 'singapore'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('malaysia', 0.647721529006958),\n",
       " ('hong_kong', 0.6318016052246094),\n",
       " ('malaysian', 0.6025472283363342),\n",
       " ('australia', 0.597445011138916),\n",
       " ('uae', 0.5960760116577148),\n",
       " ('india', 0.5947676301002502),\n",
       " ('uk', 0.5883470773696899),\n",
       " ('chinese', 0.5872371792793274),\n",
       " ('usa', 0.583607017993927),\n",
       " ('simon', 0.5695350766181946)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.most_similar('singapore', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('russia', 0.6269471049308777),\n",
       " ('korean', 0.6154767870903015),\n",
       " ('koreans', 0.6001532673835754),\n",
       " ('seoul', 0.5999401211738586),\n",
       " ('africa', 0.5899536609649658),\n",
       " ('south_korea', 0.5762559771537781),\n",
       " ('japan', 0.5648359060287476),\n",
       " ('Koreaâ_€_™', 0.5625478625297546),\n",
       " ('chinese', 0.5616578459739685),\n",
       " ('germany', 0.5582996606826782)]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.most_similar('korea', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('japanese', 0.6607722043991089),\n",
       " ('tokyo', 0.6265655755996704),\n",
       " ('america', 0.6033485531806946),\n",
       " ('europe', 0.5962790250778198),\n",
       " ('germany', 0.5782293081283569),\n",
       " ('chinese', 0.5763071179389954),\n",
       " ('india', 0.5745143294334412),\n",
       " ('hawaii', 0.5731386542320251),\n",
       " ('usa', 0.5680993795394897),\n",
       " ('korea', 0.5648358464241028)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.most_similar('japan', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dinnerware', 0.6587947607040405),\n",
       " ('crockery', 0.6426127552986145),\n",
       " ('porcelain', 0.6392654776573181),\n",
       " ('crystal_stemware', 0.6264337301254272),\n",
       " ('chinaware', 0.6146420240402222),\n",
       " ('china_plates', 0.6145730018615723),\n",
       " ('silver_flatware', 0.6102818846702576),\n",
       " ('flatware', 0.6089655756950378),\n",
       " ('bone_china', 0.6068581938743591),\n",
       " ('tableware', 0.5923404693603516)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.most_similar('china', topn=10)\n",
    "#china can be an ambiguous word. It can be a country or a material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* With Word2Vec, having a sense of the context is possible.\n",
    ">* Another function that WordVec provides is to subtract specific words (vectors) from other words (vectors) for analogy tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* For instance, let's say we are interested in what is compatible with the meaning of `king` when subtracting `man` but instead adding `woman`.\n",
    ">* Humans know that the answer is `queen`.\n",
    ">* `king` - `man` = `queen` - `woman`\n",
    ">* `king` - `man` + `woman` = `queen`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071),\n",
       " ('monarch', 0.6189674735069275),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321243286133),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.5181134343147278),\n",
       " ('sultan', 0.5098593235015869),\n",
       " ('monarchy', 0.5087411403656006)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.most_similar(positive=['king', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Alternatively, you can subtract the vectors directly instead of using `positive` and `negative` parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* `France` - `Paris` = `Italy` - `Rome`\n",
    ">* `France` - `Paris` + `Rome` = `Italy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Italy', 0.7115296125411987),\n",
       " ('Rome', 0.7092385292053223),\n",
       " ('France', 0.5904253721237183),\n",
       " ('Sicily', 0.5600441694259644),\n",
       " ('Italians', 0.5599856376647949),\n",
       " ('Flaminio_Stadium', 0.5327231287956238),\n",
       " ('Bambino_Gesu_Hospital', 0.505158007144928),\n",
       " ('Italian', 0.4975103735923767),\n",
       " ('Spain', 0.49529916048049927),\n",
       " ('Antonio_Martino', 0.4828406572341919)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w=vector['France']-vector['Paris']+vector['Rome']\n",
    "vector.most_similar(np.array([w]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('South_Korea', 0.867038905620575),\n",
       " ('Korea', 0.8067482709884644),\n",
       " ('Seoul', 0.7641376852989197),\n",
       " ('South_Korean', 0.7190972566604614),\n",
       " ('Korean', 0.6862273216247559),\n",
       " ('Japan', 0.645173192024231),\n",
       " ('North_Korea', 0.6439769864082336),\n",
       " ('Koreans', 0.6212112903594971),\n",
       " ('Yonhap', 0.619912326335907),\n",
       " ('Pyongyang', 0.6188929677009583)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w=vector['Japan']-vector['Tokyo']+vector['Seoul']\n",
    "vector.most_similar(np.array([w]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* `vector_size` indicates the dimension of the vector.\n",
    ">* `window` is the maximum distance between the current and predicted word within a sentence.\n",
    ">* `min_count` is the minimum number of occurrences of a word within the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Word2Vec([row for row in content['lemmatizer_token']], vector_size=100, min_count=1, window=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's quickly check what is the most frequent unigram so that we can check the word embedding that is actually in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('http', 2162),\n",
       " ('today', 784),\n",
       " ('house', 435),\n",
       " ('amp', 431),\n",
       " ('great', 396),\n",
       " ('new', 361),\n",
       " ('bill', 324),\n",
       " ('president', 317),\n",
       " ('act', 294),\n",
       " ('congress', 289)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([item for row in content['punct_tokenized_unigrams'] for item in row]).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amp', 0.9997757077217102),\n",
       " ('give', 0.9997571110725403),\n",
       " ('get', 0.9997456073760986),\n",
       " ('also', 0.9997426867485046),\n",
       " ('time', 0.9997410178184509),\n",
       " ('say', 0.9997406601905823),\n",
       " ('take', 0.9997357726097107),\n",
       " ('first', 0.9997175931930542),\n",
       " ('must', 0.9997175335884094),\n",
       " ('stop', 0.9997149705886841)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model.wv.most_similar(positive=['congress'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('administration', 0.9995152354240417),\n",
       " ('say', 0.9994564056396484),\n",
       " ('obamacare', 0.9994329810142517),\n",
       " ('plan', 0.999427080154419),\n",
       " ('county', 0.9994122385978699),\n",
       " ('want', 0.9994081258773804),\n",
       " ('amp', 0.9994051456451416),\n",
       " ('go', 0.9994004964828491),\n",
       " ('show', 0.9993847608566284),\n",
       " ('need', 0.9993702173233032)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['president'], topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('legislation', 0.9996969103813171),\n",
       " ('vote', 0.999688982963562),\n",
       " ('bipartisan', 0.9996677041053772),\n",
       " ('say', 0.9996665120124817),\n",
       " ('support', 0.9996650815010071),\n",
       " ('act', 0.999660074710846),\n",
       " ('take', 0.9996511340141296),\n",
       " ('would', 0.9996363520622253),\n",
       " ('amp', 0.999620258808136),\n",
       " ('senate', 0.9996134638786316)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['bill'], topn=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "is6750",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
