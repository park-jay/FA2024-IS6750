{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* We will train the hate speech classifier by training annotated dataset by De Gibert et al. (2018)\n",
    ">* First, you'll need to clone the repository from the github in the link below\n",
    ">* https://github.com/Vicomtech/hate-speech-dataset\n",
    "\n",
    ">* In your terminal, go to the directory where you want to clone the repository. To see where you are at, `ls -l`, and `cd ..` to go to the upper level (parent directory), and `cd [NAME OF THE DIRECTORY]` to go to the directory you want to go to. Then, `git clone https://github.com/Vicomtech/hate-speech-dataset.git`\n",
    "\n",
    ">* You will find the dataset in the `annotations_metadata.csv` file\n",
    ">* Let's read the csv file and have a glance at the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv('../hate-speech-dataset/annotations_metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>subforum_id</th>\n",
       "      <th>num_contexts</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12834217_1</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12834217_2</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12834217_3</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12834217_4</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12834217_5</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      file_id  user_id  subforum_id  num_contexts   label\n",
       "0  12834217_1   572066         1346             0  noHate\n",
       "1  12834217_2   572066         1346             0  noHate\n",
       "2  12834217_3   572066         1346             0  noHate\n",
       "3  12834217_4   572066         1346             0    hate\n",
       "4  12834217_5   572066         1346             0  noHate"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['noHate', 'hate', 'idk/skip', 'relation'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "noHate      9507\n",
       "hate        1196\n",
       "relation     168\n",
       "idk/skip      73\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>subforum_id</th>\n",
       "      <th>num_contexts</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12834217_1</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12834217_2</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12834217_3</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12834217_5</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12834217_6</td>\n",
       "      <td>572066</td>\n",
       "      <td>1346</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10938</th>\n",
       "      <td>33676864_4</td>\n",
       "      <td>734541</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10939</th>\n",
       "      <td>33676864_5</td>\n",
       "      <td>734541</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10940</th>\n",
       "      <td>33677019_1</td>\n",
       "      <td>735154</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10941</th>\n",
       "      <td>33677019_2</td>\n",
       "      <td>735154</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10943</th>\n",
       "      <td>33677053_2</td>\n",
       "      <td>572266</td>\n",
       "      <td>1388</td>\n",
       "      <td>0</td>\n",
       "      <td>noHate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9507 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          file_id  user_id  subforum_id  num_contexts   label\n",
       "0      12834217_1   572066         1346             0  noHate\n",
       "1      12834217_2   572066         1346             0  noHate\n",
       "2      12834217_3   572066         1346             0  noHate\n",
       "4      12834217_5   572066         1346             0  noHate\n",
       "5      12834217_6   572066         1346             0  noHate\n",
       "...           ...      ...          ...           ...     ...\n",
       "10938  33676864_4   734541         1388             0  noHate\n",
       "10939  33676864_5   734541         1388             0  noHate\n",
       "10940  33677019_1   735154         1388             0  noHate\n",
       "10941  33677019_2   735154         1388             0  noHate\n",
       "10943  33677053_2   572266         1388             0  noHate\n",
       "\n",
       "[9507 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['label']=='noHate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training=data[(data['label']=='noHate')|(data['label']=='hate')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "noHate    9507\n",
       "hate      1196\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/r8z51q092v566bd_r0g_zj640000gn/T/ipykernel_2502/755911698.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training['class']=training['label'].apply(lambda x: 1 if x=='hate' else 0)\n"
     ]
    }
   ],
   "source": [
    "training['class']=training['label'].apply(lambda x: 1 if x=='hate' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class\n",
       "0    9507\n",
       "1    1196\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training['class'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* We will import `os` to get the name of the files in the training dataset folder from the repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10944"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir('../hate-speech-dataset/all_files'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* We will call each file and read the text in the file and create a dictionary with the file name (key) and the text (value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dict={}\n",
    "for file in os.listdir('../hate-speech-dataset/all_files'):\n",
    "    with open('../hate-speech-dataset/all_files/'+file, 'r') as content:\n",
    "        all_dict[file.strip('.txt')]=content.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10944"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/r8z51q092v566bd_r0g_zj640000gn/T/ipykernel_2502/856071315.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  training['text']=training['file_id'].apply(lambda x: all_dict[x])\n"
     ]
    }
   ],
   "source": [
    "training['text']=training['file_id'].apply(lambda x: all_dict[x])\n",
    "#This will populate the text column with the content of the file in accordance with \n",
    "#the file_id column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Once we have the class (e.g., 0 for non-hate speech, 1 for hate speech), we will train the classifier using the `text` column and the `class` column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* With the training dataset, let's classify the Parler dataset from week 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parler=pd.read_csv('../week6/subset-2021-01-11-voter_fraud.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['article', 'body', 'bodywithurls', 'comments', 'createdAt',\n",
       "       'createdAtformatted', 'creator', 'datatype', 'depth', 'depthRaw',\n",
       "       ...\n",
       "       'urls.15.createdAt', 'urls.15.domain', 'urls.15.id', 'urls.15.long',\n",
       "       'urls.15.metadata.length', 'urls.15.metadata.mimeType',\n",
       "       'urls.15.metadata.site', 'urls.15.modified', 'urls.15.short',\n",
       "       'urls.15.state'],\n",
       "      dtype='object', length=420)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parler.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "content=parler[['body','username']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Folks may be our last posts. Pogilosi just ask...</td>\n",
       "      <td>Terryb158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oh notre they want to investigate something. L...</td>\n",
       "      <td>AlanBond7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>White trash pieces of shit who are going to be...</td>\n",
       "      <td>Tifdog11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>He has never lived in a totalitarian country; ...</td>\n",
       "      <td>AlexaImmigrant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>White trash pieces of shit who are going to be...</td>\n",
       "      <td>Tifdog11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>GRAHAM\\nHe begged us for money on Hannity and ...</td>\n",
       "      <td>Millsfarms</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Neither is election fraud.</td>\n",
       "      <td>xfitnesscoach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Investigation of fraudulent voting practices, ...</td>\n",
       "      <td>Bobbfishen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Why did Nunes recuse himself on the election f...</td>\n",
       "      <td>BarryOKenyan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>White trash pieces of shit who are going to be...</td>\n",
       "      <td>Tifdog11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 body        username\n",
       "0   Folks may be our last posts. Pogilosi just ask...       Terryb158\n",
       "1   Oh notre they want to investigate something. L...       AlanBond7\n",
       "2   White trash pieces of shit who are going to be...        Tifdog11\n",
       "3   He has never lived in a totalitarian country; ...  AlexaImmigrant\n",
       "4   White trash pieces of shit who are going to be...        Tifdog11\n",
       "..                                                ...             ...\n",
       "86  GRAHAM\\nHe begged us for money on Hannity and ...      Millsfarms\n",
       "87                         Neither is election fraud.   xfitnesscoach\n",
       "88  Investigation of fraudulent voting practices, ...      Bobbfishen\n",
       "89  Why did Nunes recuse himself on the election f...    BarryOKenyan\n",
       "90  White trash pieces of shit who are going to be...        Tifdog11\n",
       "\n",
       "[91 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* When we think about the classification task, we need to think about `what to classify` and `how to classify`\n",
    ">* `What to classify`: unigram count vectorizer, tf-idf vectorizer\n",
    ">* `How to classify`: Naive Bayes, Support Vector Machine (SVM), and Logistic Regression (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Convert the text into a vector representation for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_count_vectorizer=CountVectorizer(encoding='latin-1', ngram_range=(1,1))\n",
    "tfidf_vectorizer=TfidfVectorizer(encoding='latin-1', ngram_range=(1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1=unigram_count_vectorizer.fit_transform(training['text'])\n",
    "#prepare training data as a vector representation   \n",
    "X1_predict=unigram_count_vectorizer.transform(content['body'])\n",
    "#prepare the target as a vector representation\n",
    "X1_tfidf=tfidf_vectorizer.fit_transform(training['text'])\n",
    "#prepare training data as a vector representation\n",
    "X1_tfidf_predict=tfidf_vectorizer.transform(content['body'])\n",
    "#prepare the target as a vector representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10703, 15871)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1.toarray().shape\n",
    "#each row represents a document and \n",
    "#the length is determined by the number of unique words in the corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91, 15871)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1_predict.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's train the `Multinomial Naive Bayes classifier` with the `Unigram Count Vectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb=MultinomialNB().fit(X1, training['class'])\n",
    "nb.predict(X1_predict)\n",
    "#only index 42 has been classified as hate speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_numbers=[i for i in range(len(nb.predict(X1_predict))) if nb.predict(X1_predict)[i]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Greece need a man like Adolf Hitler and now is the perfect time to start a '\n",
      " 'national political movement to free Your country from the zionist bankers')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(training['text'].iloc[index_numbers[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's train the `Support Vector Machine (SVM) classifier` with the `Unigram Count Vectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 1])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc=SVC(kernel='linear').fit(X1, training['class'])\n",
    "svc.predict(X1_predict)\n",
    "#interestingly, the svm model flagged more content as hate speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "index_numbers = [i for i, num in enumerate(svc.predict(X1_predict)) if num == 1]\n",
    "print(len(index_numbers))\n",
    "#27 contents have been flagged as hate speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     White trash pieces of shit who are going to be...\n",
       "4     White trash pieces of shit who are going to be...\n",
       "6     White trash pieces of shit who are going to be...\n",
       "8     Well they already made sure of that when they ...\n",
       "16    Those sleazbags dems are trying to pull off a ...\n",
       "19    White trash pieces of shit who are going to be...\n",
       "26    Traitor seems a relative term to you. It seems...\n",
       "28    why bother if they get away with this election...\n",
       "29    White trash pieces of shit who are going to be...\n",
       "32    White trash pieces of shit who are going to be...\n",
       "33    White trash pieces of shit who are going to be...\n",
       "36    White trash pieces of shit who are going to be...\n",
       "44    White trash pieces of shit who are going to be...\n",
       "49    White trash pieces of shit who are going to be...\n",
       "51    @halfcrazy First of all I'm.not a guy..duh..se...\n",
       "53    White trash pieces of shit who are going to be...\n",
       "60    White trash pieces of shit who are going to be...\n",
       "64    Ya'll need to stop believing this shit . Witho...\n",
       "65    White trash pieces of shit who are going to be...\n",
       "66    @HonyBadger White trash pieces of shit who are...\n",
       "68    White trash pieces of shit who are going to be...\n",
       "72    No, we did not. As with the general election, ...\n",
       "75    White trash pieces of shit who are going to be...\n",
       "80    White trash pieces of shit who are going to be...\n",
       "83    White trash pieces of shit who are going to be...\n",
       "84    I guess I missed the part where the Speaker of...\n",
       "90    White trash pieces of shit who are going to be...\n",
       "Name: body, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['body'].iloc[index_numbers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's train the `Logistic Regression (LR) classifier` with the `Unigram Count Vectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mstudio/miniconda3/envs/is6750/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=LogisticRegression().fit(X1, training['class'])\n",
    "lr.predict(X1_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "index_numbers = [i for i, num in enumerate(lr.predict(X1_predict)) if num == 1]\n",
    "print(len(index_numbers)) \n",
    "#27 contents have been flagged as hate speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Folks may be our last posts. Pogilosi just ask...\n",
       "2     White trash pieces of shit who are going to be...\n",
       "4     White trash pieces of shit who are going to be...\n",
       "6     White trash pieces of shit who are going to be...\n",
       "16    Those sleazbags dems are trying to pull off a ...\n",
       "19    White trash pieces of shit who are going to be...\n",
       "26    Traitor seems a relative term to you. It seems...\n",
       "28    why bother if they get away with this election...\n",
       "29    White trash pieces of shit who are going to be...\n",
       "32    White trash pieces of shit who are going to be...\n",
       "33    White trash pieces of shit who are going to be...\n",
       "36    White trash pieces of shit who are going to be...\n",
       "44    White trash pieces of shit who are going to be...\n",
       "47    Couldn't even watch all of this. WTF? Well, le...\n",
       "49    White trash pieces of shit who are going to be...\n",
       "51    @halfcrazy First of all I'm.not a guy..duh..se...\n",
       "53    White trash pieces of shit who are going to be...\n",
       "60    White trash pieces of shit who are going to be...\n",
       "64    Ya'll need to stop believing this shit . Witho...\n",
       "65    White trash pieces of shit who are going to be...\n",
       "66    @HonyBadger White trash pieces of shit who are...\n",
       "68    White trash pieces of shit who are going to be...\n",
       "75    White trash pieces of shit who are going to be...\n",
       "80    White trash pieces of shit who are going to be...\n",
       "83    White trash pieces of shit who are going to be...\n",
       "84    I guess I missed the part where the Speaker of...\n",
       "90    White trash pieces of shit who are going to be...\n",
       "Name: body, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['body'].iloc[index_numbers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* If we looked at the classifiers with `Unigram Count Vectorizer`, we can also look at the classifiers with `Tf-idf Vectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb=MultinomialNB().fit(X1_tfidf, training['class'])\n",
    "nb.predict(X1_predict)\n",
    "#Interestingly, the naive bayes model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* `Multinomial Naive Bayes` with `Tf-idf Vectorizer` did not classify the hate speech at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's train the `Support Vector Machine (SVM) classifier` with the `Tf-idf Vectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc=SVC(kernel='linear').fit(X1_tfidf, training['class'])\n",
    "svc.predict(X1_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_numbers = [i for i, num in enumerate(svc.predict(X1_predict)) if num == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_numbers)\n",
    "#how many contents have been flagged as hate speech? 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Folks may be our last posts. Pogilosi just ask...\n",
       "1     Oh notre they want to investigate something. L...\n",
       "2     White trash pieces of shit who are going to be...\n",
       "3     He has never lived in a totalitarian country; ...\n",
       "4     White trash pieces of shit who are going to be...\n",
       "                            ...                        \n",
       "84    I guess I missed the part where the Speaker of...\n",
       "85    Oh now they want to investigate something. Let...\n",
       "86    GRAHAM\\nHe begged us for money on Hannity and ...\n",
       "88    Investigation of fraudulent voting practices, ...\n",
       "90    White trash pieces of shit who are going to be...\n",
       "Name: body, Length: 65, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['body'].iloc[index_numbers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's train the `Logistic Regression (LR) classifier` with the `Tf-idf Vectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       1, 0, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr=LogisticRegression().fit(X1_tfidf, training['class'])\n",
    "lr.predict(X1_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_numbers = [i for i, num in enumerate(lr.predict(X1_predict)) if num == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_numbers)\n",
    "#how many contents have been flagged as hate speech? 78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Folks may be our last posts. Pogilosi just ask...\n",
       "1     Oh notre they want to investigate something. L...\n",
       "2     White trash pieces of shit who are going to be...\n",
       "3     He has never lived in a totalitarian country; ...\n",
       "4     White trash pieces of shit who are going to be...\n",
       "                            ...                        \n",
       "84    I guess I missed the part where the Speaker of...\n",
       "85    Oh now they want to investigate something. Let...\n",
       "86    GRAHAM\\nHe begged us for money on Hannity and ...\n",
       "88    Investigation of fraudulent voting practices, ...\n",
       "90    White trash pieces of shit who are going to be...\n",
       "Name: body, Length: 78, dtype: object"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['body'].iloc[index_numbers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* We saw that different combinations of vectorizers and classifiers have different performance. Can we evaluate the performance of the classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* If we want to evaluate the performance of the classifier, we need to have the annotated test dataset.\n",
    ">* Let's use the hate speech dataset from De Gibert et al. (2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* We have to split the entire annotated dataset into training and testing datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's train the `Multinomial Naive Bayes classifier` with the `Unigram Count Vectorizer` and evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_count=unigram_count_vectorizer.fit_transform(training['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test=train_test_split(training['text'], training['class'], test_size=0.2, random_state=42)\n",
    "#usually 80% of the data is used for training and 20% for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unigram=unigram_count_vectorizer.transform(X_train)\n",
    "X_test_unigram=unigram_count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9046    0.9654    0.9340      1906\n",
      "           1     0.3832    0.1745    0.2398       235\n",
      "\n",
      "    accuracy                         0.8786      2141\n",
      "   macro avg     0.6439    0.5699    0.5869      2141\n",
      "weighted avg     0.8474    0.8786    0.8578      2141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb=MultinomialNB().fit(X_train_unigram, y_train)\n",
    "y_nb=nb.predict(X_test_unigram)\n",
    "print(metrics.classification_report(y_test, y_nb, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's train the `Support Vector Machine (SVM) classifier` with the `Unigram Count Vectorizer` and evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9287    0.9496    0.9390      1906\n",
      "           1     0.5000    0.4085    0.4496       235\n",
      "\n",
      "    accuracy                         0.8902      2141\n",
      "   macro avg     0.7143    0.6791    0.6943      2141\n",
      "weighted avg     0.8816    0.8902    0.8853      2141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc=SVC(kernel='linear').fit(X_train_unigram, y_train)\n",
    "y_svc=svc.predict(X_test_unigram)\n",
    "print(metrics.classification_report(y_test, y_svc, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Lastly, let's train the `Logistic Regression (LR) classifier` with the `Unigram Count Vectorizer` and evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9197    0.9738    0.9460      1906\n",
      "           1     0.5935    0.3106    0.4078       235\n",
      "\n",
      "    accuracy                         0.9010      2141\n",
      "   macro avg     0.7566    0.6422    0.6769      2141\n",
      "weighted avg     0.8839    0.9010    0.8869      2141\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mstudio/miniconda3/envs/is6750/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "lr=LogisticRegression().fit(X_train_unigram, y_train)\n",
    "y_lr=lr.predict(X_test_unigram)\n",
    "print(metrics.classification_report(y_test, y_lr, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* We can also evaluate the performance of the classifiers with `Tf-idf Vectorizer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf=tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf=tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's train the `Multinomial Naive Bayes classifier` with the `Tf-idf Vectorizer` and evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8902    1.0000    0.9419      1906\n",
      "           1     0.0000    0.0000    0.0000       235\n",
      "\n",
      "    accuracy                         0.8902      2141\n",
      "   macro avg     0.4451    0.5000    0.4710      2141\n",
      "weighted avg     0.7925    0.8902    0.8385      2141\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mstudio/miniconda3/envs/is6750/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mstudio/miniconda3/envs/is6750/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mstudio/miniconda3/envs/is6750/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "nb=MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "y_nb=nb.predict(X_test_tfidf)\n",
    "print(metrics.classification_report(y_test, y_nb, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's train the `Support Vector Machine (SVM) classifier` with the `Tf-idf Vectorizer` and evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9022    0.9969    0.9472      1906\n",
      "           1     0.8286    0.1234    0.2148       235\n",
      "\n",
      "    accuracy                         0.9010      2141\n",
      "   macro avg     0.8654    0.5601    0.5810      2141\n",
      "weighted avg     0.8941    0.9010    0.8668      2141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc=SVC(kernel='linear').fit(X_train_tfidf, y_train)\n",
    "y_svc=svc.predict(X_test_tfidf)\n",
    "print(metrics.classification_report(y_test, y_svc, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Let's train the `Logistic Regression (LR) classifier` with the `Tf-idf Vectorizer` and evaluate the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8972    0.9979    0.9449      1906\n",
      "           1     0.8095    0.0723    0.1328       235\n",
      "\n",
      "    accuracy                         0.8963      2141\n",
      "   macro avg     0.8533    0.5351    0.5388      2141\n",
      "weighted avg     0.8875    0.8963    0.8557      2141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr=LogisticRegression().fit(X_train_tfidf, y_train)\n",
    "y_lr=lr.predict(X_test_tfidf)\n",
    "print(metrics.classification_report(y_test, y_lr, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Q. What do you think about the metrics of the classifiers? Do you think the metrics are good enough to explain the performance of the classifiers when applied to the unseen data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* If we looked at the classification of binary classes, which are (1) hate speech or (2) non-hate speech, we can also measure the degree of toxicity of social media content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Google's Perspective API provides a toxicity score for the text. The toxicity score is a value between 0 and 1, where 0 means not toxic and 1 means toxic.\n",
    ">* https://perspectiveapi.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* In order to use Perspective API, you need to have an access to the API key.\n",
    ">* You can request the API key by following the instructions in the link below\n",
    ">* https://developers.perspectiveapi.com/s/docs-enable-the-api?language=en_US"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* We will need to install the `PyPerspective` package which is a Python wrapper for the Perspective API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install PyPerspective\n",
    "from PyPerspective.Perspective import Perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Among various attributes, we will use the `TOXICITY` and `IDENTITY_ATTACK` attributes to measure the toxicity of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPerspective.Perspective import Perspective\n",
    "perspective=Perspective(\"\", ratelimit=False, default_not_store=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tox_measure(df):\n",
    "    import time\n",
    "    toxicity_result=[]\n",
    "    identity_result=[]\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            scores=perspective.get_score(row['body'], tests=['TOXICITY', 'IDENTITY_ATTACK'], langs=['en'])\n",
    "            toxicity_result.append(scores['TOXICITY'].score)\n",
    "            identity_result.append(scores['IDENTITY_ATTACK'].score)\n",
    "            time.sleep(2)\n",
    "            #print(idx)\n",
    "        except KeyError:\n",
    "            toxicity_result.append(0)\n",
    "            identity_result.append(0)\n",
    "            time.sleep(2)          \n",
    "# #             print(idx)\n",
    "    df['toxicity']=toxicity_result\n",
    "    df['identity']=identity_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3_/r8z51q092v566bd_r0g_zj640000gn/T/ipykernel_2502/136488992.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['toxicity']=toxicity_result\n",
      "/var/folders/3_/r8z51q092v566bd_r0g_zj640000gn/T/ipykernel_2502/136488992.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['identity']=identity_result\n"
     ]
    }
   ],
   "source": [
    "tox_measure(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>username</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>identity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Folks may be our last posts. Pogilosi just ask...</td>\n",
       "      <td>Terryb158</td>\n",
       "      <td>0.292228</td>\n",
       "      <td>0.026610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Oh notre they want to investigate something. L...</td>\n",
       "      <td>AlanBond7</td>\n",
       "      <td>0.457579</td>\n",
       "      <td>0.010581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>White trash pieces of shit who are going to be...</td>\n",
       "      <td>Tifdog11</td>\n",
       "      <td>0.950486</td>\n",
       "      <td>0.717202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>He has never lived in a totalitarian country; ...</td>\n",
       "      <td>AlexaImmigrant</td>\n",
       "      <td>0.175451</td>\n",
       "      <td>0.015355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>White trash pieces of shit who are going to be...</td>\n",
       "      <td>Tifdog11</td>\n",
       "      <td>0.950486</td>\n",
       "      <td>0.717202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>GRAHAM\\nHe begged us for money on Hannity and ...</td>\n",
       "      <td>Millsfarms</td>\n",
       "      <td>0.394416</td>\n",
       "      <td>0.049511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>Neither is election fraud.</td>\n",
       "      <td>xfitnesscoach</td>\n",
       "      <td>0.038520</td>\n",
       "      <td>0.002553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>Investigation of fraudulent voting practices, ...</td>\n",
       "      <td>Bobbfishen</td>\n",
       "      <td>0.052553</td>\n",
       "      <td>0.003200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>Why did Nunes recuse himself on the election f...</td>\n",
       "      <td>BarryOKenyan</td>\n",
       "      <td>0.075294</td>\n",
       "      <td>0.003644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>White trash pieces of shit who are going to be...</td>\n",
       "      <td>Tifdog11</td>\n",
       "      <td>0.950486</td>\n",
       "      <td>0.717202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 body        username  \\\n",
       "0   Folks may be our last posts. Pogilosi just ask...       Terryb158   \n",
       "1   Oh notre they want to investigate something. L...       AlanBond7   \n",
       "2   White trash pieces of shit who are going to be...        Tifdog11   \n",
       "3   He has never lived in a totalitarian country; ...  AlexaImmigrant   \n",
       "4   White trash pieces of shit who are going to be...        Tifdog11   \n",
       "..                                                ...             ...   \n",
       "86  GRAHAM\\nHe begged us for money on Hannity and ...      Millsfarms   \n",
       "87                         Neither is election fraud.   xfitnesscoach   \n",
       "88  Investigation of fraudulent voting practices, ...      Bobbfishen   \n",
       "89  Why did Nunes recuse himself on the election f...    BarryOKenyan   \n",
       "90  White trash pieces of shit who are going to be...        Tifdog11   \n",
       "\n",
       "    toxicity  identity  \n",
       "0   0.292228  0.026610  \n",
       "1   0.457579  0.010581  \n",
       "2   0.950486  0.717202  \n",
       "3   0.175451  0.015355  \n",
       "4   0.950486  0.717202  \n",
       "..       ...       ...  \n",
       "86  0.394416  0.049511  \n",
       "87  0.038520  0.002553  \n",
       "88  0.052553  0.003200  \n",
       "89  0.075294  0.003644  \n",
       "90  0.950486  0.717202  \n",
       "\n",
       "[91 rows x 4 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    91.000000\n",
       "mean      0.403556\n",
       "std       0.334644\n",
       "min       0.014639\n",
       "25%       0.122601\n",
       "50%       0.275254\n",
       "75%       0.712751\n",
       "max       0.950486\n",
       "Name: toxicity, dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content['toxicity'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Frequency'>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "content.plot.hist(y='toxicity', bins=10, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>username</th>\n",
       "      <th>toxicity</th>\n",
       "      <th>identity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>White trash pieces of shit who are going to be...</td>\n",
       "      <td>Tifdog11</td>\n",
       "      <td>0.950486</td>\n",
       "      <td>0.717202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>White trash pieces of shit who are going to be...</td>\n",
       "      <td>Tifdog11</td>\n",
       "      <td>0.950486</td>\n",
       "      <td>0.717202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>White trash pieces of shit who are going to be...</td>\n",
       "      <td>Tifdog11</td>\n",
       "      <td>0.950486</td>\n",
       "      <td>0.717202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>White trash pieces of shit who are going to be...</td>\n",
       "      <td>Tifdog11</td>\n",
       "      <td>0.950486</td>\n",
       "      <td>0.717202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>White trash pieces of shit who are going to be...</td>\n",
       "      <td>Tifdog11</td>\n",
       "      <td>0.950486</td>\n",
       "      <td>0.717202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>White trash pieces of shit who are going to be...</td>\n",
       "      <td>Tifdog11</td>\n",
       "      <td>0.950486</td>\n",
       "      <td>0.717202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>White trash pieces of shit who are going to be...</td>\n",
       "      <td>Tifdog11</td>\n",
       "      <td>0.950486</td>\n",
       "      <td>0.717202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>White trash pieces of shit who are going to be...</td>\n",
       "      <td>Tifdog11</td>\n",
       "      <td>0.950486</td>\n",
       "      <td>0.717202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>White trash pieces of shit who are going to be...</td>\n",
       "      <td>Tifdog11</td>\n",
       "      <td>0.950486</td>\n",
       "      <td>0.717202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>White trash pieces of shit who are going to be...</td>\n",
       "      <td>Tifdog11</td>\n",
       "      <td>0.950486</td>\n",
       "      <td>0.717202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>White trash pieces of shit who are going to be...</td>\n",
       "      <td>Tifdog11</td>\n",
       "      <td>0.950486</td>\n",
       "      <td>0.717202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>White trash pieces of shit who are going to be...</td>\n",
       "      <td>Tifdog11</td>\n",
       "      <td>0.950486</td>\n",
       "      <td>0.717202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>White trash pieces of shit who are going to be...</td>\n",
       "      <td>Tifdog11</td>\n",
       "      <td>0.950486</td>\n",
       "      <td>0.717202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>@HonyBadger White trash pieces of shit who are...</td>\n",
       "      <td>Tifdog11</td>\n",
       "      <td>0.916254</td>\n",
       "      <td>0.600718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>White trash pieces of shit who are going to be...</td>\n",
       "      <td>Tifdog11</td>\n",
       "      <td>0.950486</td>\n",
       "      <td>0.717202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>White trash pieces of shit who are going to be...</td>\n",
       "      <td>Tifdog11</td>\n",
       "      <td>0.950486</td>\n",
       "      <td>0.717202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>White trash pieces of shit who are going to be...</td>\n",
       "      <td>Tifdog11</td>\n",
       "      <td>0.950486</td>\n",
       "      <td>0.717202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>White trash pieces of shit who are going to be...</td>\n",
       "      <td>Tifdog11</td>\n",
       "      <td>0.950486</td>\n",
       "      <td>0.717202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>White trash pieces of shit who are going to be...</td>\n",
       "      <td>Tifdog11</td>\n",
       "      <td>0.950486</td>\n",
       "      <td>0.717202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 body  username  toxicity  \\\n",
       "2   White trash pieces of shit who are going to be...  Tifdog11  0.950486   \n",
       "4   White trash pieces of shit who are going to be...  Tifdog11  0.950486   \n",
       "6   White trash pieces of shit who are going to be...  Tifdog11  0.950486   \n",
       "19  White trash pieces of shit who are going to be...  Tifdog11  0.950486   \n",
       "29  White trash pieces of shit who are going to be...  Tifdog11  0.950486   \n",
       "32  White trash pieces of shit who are going to be...  Tifdog11  0.950486   \n",
       "33  White trash pieces of shit who are going to be...  Tifdog11  0.950486   \n",
       "36  White trash pieces of shit who are going to be...  Tifdog11  0.950486   \n",
       "44  White trash pieces of shit who are going to be...  Tifdog11  0.950486   \n",
       "49  White trash pieces of shit who are going to be...  Tifdog11  0.950486   \n",
       "53  White trash pieces of shit who are going to be...  Tifdog11  0.950486   \n",
       "60  White trash pieces of shit who are going to be...  Tifdog11  0.950486   \n",
       "65  White trash pieces of shit who are going to be...  Tifdog11  0.950486   \n",
       "66  @HonyBadger White trash pieces of shit who are...  Tifdog11  0.916254   \n",
       "68  White trash pieces of shit who are going to be...  Tifdog11  0.950486   \n",
       "75  White trash pieces of shit who are going to be...  Tifdog11  0.950486   \n",
       "80  White trash pieces of shit who are going to be...  Tifdog11  0.950486   \n",
       "83  White trash pieces of shit who are going to be...  Tifdog11  0.950486   \n",
       "90  White trash pieces of shit who are going to be...  Tifdog11  0.950486   \n",
       "\n",
       "    identity  \n",
       "2   0.717202  \n",
       "4   0.717202  \n",
       "6   0.717202  \n",
       "19  0.717202  \n",
       "29  0.717202  \n",
       "32  0.717202  \n",
       "33  0.717202  \n",
       "36  0.717202  \n",
       "44  0.717202  \n",
       "49  0.717202  \n",
       "53  0.717202  \n",
       "60  0.717202  \n",
       "65  0.717202  \n",
       "66  0.600718  \n",
       "68  0.717202  \n",
       "75  0.717202  \n",
       "80  0.717202  \n",
       "83  0.717202  \n",
       "90  0.717202  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[content['toxicity']>0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('White trash pieces of shit who are going to be investigated for inciting a '\n",
      " 'riot, sedition and conspiracy to commit election fraud. Go fuck yourselves. '\n",
      " 'Weâ€™re gonna take away your secret service protection, then letâ€™s see how '\n",
      " 'brave you are, you short dick cheap fuck. Just like that fat piece of shit '\n",
      " 'your Dad is. I hope you share a cell with a HUGE black guy who treats you '\n",
      " 'like every animal youâ€™ve killed. We will all laugh. Go suck you off Putin '\n",
      " 'so you have a place to escape to, you fucking loser. '\n",
      " 'ðŸ’©ðŸ–•ðŸ\\x8f»ðŸ–•ðŸ\\x8f»ðŸ–•ðŸ\\x8f»ðŸ–•ðŸ\\x8f»ðŸ–•ðŸ\\x8f»ðŸ–•ðŸ\\x8f»ðŸ–•ðŸ\\x8f»ðŸ–•ðŸ\\x8f»ðŸ–•ðŸ\\x8f»ðŸ–•ðŸ\\x8f»ðŸ–•ðŸ\\x8f»')\n"
     ]
    }
   ],
   "source": [
    "pprint(content.iloc[2]['body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">* Q. Which one do you think works better for finding the hate speech? The Perspective API or the classifiers trained with the annotated dataset?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "is6750",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
